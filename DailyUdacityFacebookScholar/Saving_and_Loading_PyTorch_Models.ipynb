{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### %matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim \n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download = True, train = True, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True)\n",
    "\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download = True, train = False, transform = transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p = 0.5):\n",
    "        '''\n",
    "        Builds a feedforward network with arbitrary hidden layers.\n",
    "        Arguments\n",
    "        ---------\n",
    "        input_size: integer, size of the input layer\n",
    "        output_size: integer, size of the output layer \n",
    "        hidden_layers: list of integers, the sizes of the hidden layers \n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers \n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.dropout = nn.Dropout(p = drop_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass through the network, return the output logits\n",
    "        '''\n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        s = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim = 1)\n",
    "    \n",
    "def validation(model, testloader, criterion):\n",
    "    accuracy = 0\n",
    "    test_loss = 0\n",
    "    for images, labels  in testloader:\n",
    "        images = images.resize_(images.size()[0], 784)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        test_loss += criterion(output, labels).item()\n",
    "\n",
    "        ## Calculating the accracy\n",
    "        # Model's ouput is log-softmax, take exponential to get the prob.\n",
    "        ps = torch.exp(output)\n",
    "        # Class with highest probability is our predicted class, compare with true labels\n",
    "        equality = (labels.data == ps.max(1)[1])\n",
    "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "    return test_loss, accuracy \n",
    "    \n",
    "\n",
    "def train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every = 40):\n",
    "    steps = 0\n",
    "    running_loss = 0 \n",
    "    for e in range(epochs):\n",
    "        # Model in training mode, dropout is on \n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            steps +=1\n",
    "            \n",
    "            # Flatten images into a 784 long vector \n",
    "            images.resize_(images.size()[0], 784)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if steps % print_every == 0:\n",
    "                # Model in inference mode, dropout is off \n",
    "                model.eval()\n",
    "                \n",
    "                # Turn off gradients for validation, will speed up inference\n",
    "                with torch.no_grad():\n",
    "                    test_loss, accuracy = validation(model, testloader, criterion) \n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                     \"Training Loss: {:.3f}..\".format(running_loss/print_every),\n",
    "                     \"Test Loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
    "                     \"Test Accuracy: {:.3f}..\".format(accuracy/len(testloader)))\n",
    "                \n",
    "                running_loss = 0\n",
    "                \n",
    "                # Make sure dropout and grads are on for training\n",
    "                model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run fc_model.ipynb\n",
    "\n",
    "model = Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Training Loss: 3.747.. Test Loss: 2.573.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.217.. Test Loss: 2.079.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.106.. Test Loss: 2.096.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.143.. Test Loss: 1.874.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.103.. Test Loss: 1.939.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.077.. Test Loss: 1.997.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.143.. Test Loss: 2.045.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.971.. Test Loss: 1.872.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.069.. Test Loss: 1.958.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.067.. Test Loss: 1.712.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.991.. Test Loss: 1.843.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.978.. Test Loss: 1.680.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.942.. Test Loss: 1.668.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.068.. Test Loss: 1.803.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.055.. Test Loss: 1.861.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.032.. Test Loss: 1.707.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.927.. Test Loss: 1.720.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.014.. Test Loss: 1.736.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.994.. Test Loss: 1.787.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.920.. Test Loss: 1.626.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.953.. Test Loss: 1.681.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 3.009.. Test Loss: 1.713.. Test Accuracy: 0.000..\n",
      "Epoch: 1/2 Training Loss: 2.910.. Test Loss: 1.741.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 3.050.. Test Loss: 1.607.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 3.054.. Test Loss: 1.655.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.976.. Test Loss: 1.687.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.934.. Test Loss: 1.602.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 3.005.. Test Loss: 1.785.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.867.. Test Loss: 1.586.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 3.001.. Test Loss: 1.563.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 3.013.. Test Loss: 1.622.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.985.. Test Loss: 1.699.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.901.. Test Loss: 1.595.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.861.. Test Loss: 1.540.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.935.. Test Loss: 1.565.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.860.. Test Loss: 1.566.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.866.. Test Loss: 1.508.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.925.. Test Loss: 1.458.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.939.. Test Loss: 1.513.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.936.. Test Loss: 1.693.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.882.. Test Loss: 1.764.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.858.. Test Loss: 1.479.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.862.. Test Loss: 1.523.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.908.. Test Loss: 1.370.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.880.. Test Loss: 1.476.. Test Accuracy: 0.000..\n",
      "Epoch: 2/2 Training Loss: 2.822.. Test Loss: 1.348.. Test Accuracy: 0.000..\n"
     ]
    }
   ],
   "source": [
    "train(model, trainloader, testloader, criterion, optimizer, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys()\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's save the dict the state with \"torch.save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "# and we can load the saved one with \"torch.load\"\n",
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the state dict in the network, we do \"model.load_state_dict(state_dict)\"\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param of torch.Size([400, 784]) from checkpoint, where the shape is torch.Size([512, 784]) in current model.\n\tsize mismatch for hidden_layers.0.bias: copying a param of torch.Size([400]) from checkpoint, where the shape is torch.Size([512]) in current model.\n\tsize mismatch for hidden_layers.1.weight: copying a param of torch.Size([200, 400]) from checkpoint, where the shape is torch.Size([256, 512]) in current model.\n\tsize mismatch for hidden_layers.1.bias: copying a param of torch.Size([200]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for hidden_layers.2.weight: copying a param of torch.Size([100, 200]) from checkpoint, where the shape is torch.Size([128, 256]) in current model.\n\tsize mismatch for hidden_layers.2.bias: copying a param of torch.Size([100]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for output.weight: copying a param of torch.Size([10, 100]) from checkpoint, where the shape is torch.Size([10, 128]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2f9ee1ef8a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# this will throw an error bcs the tensor sizes are wrong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param of torch.Size([400, 784]) from checkpoint, where the shape is torch.Size([512, 784]) in current model.\n\tsize mismatch for hidden_layers.0.bias: copying a param of torch.Size([400]) from checkpoint, where the shape is torch.Size([512]) in current model.\n\tsize mismatch for hidden_layers.1.weight: copying a param of torch.Size([200, 400]) from checkpoint, where the shape is torch.Size([256, 512]) in current model.\n\tsize mismatch for hidden_layers.1.bias: copying a param of torch.Size([200]) from checkpoint, where the shape is torch.Size([256]) in current model.\n\tsize mismatch for hidden_layers.2.weight: copying a param of torch.Size([100, 200]) from checkpoint, where the shape is torch.Size([128, 256]) in current model.\n\tsize mismatch for hidden_layers.2.bias: copying a param of torch.Size([100]) from checkpoint, where the shape is torch.Size([128]) in current model.\n\tsize mismatch for output.weight: copying a param of torch.Size([10, 100]) from checkpoint, where the shape is torch.Size([10, 128]) in current model."
     ]
    }
   ],
   "source": [
    "# Loading the state dict works only if the model architecture is\n",
    "# exactly the same as the checkpoint architecture\n",
    "\n",
    "# Try 1\n",
    "model = Network(784, 10, [400,200,100])\n",
    "# this will throw an error bcs the tensor sizes are wrong\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We hanve to rebuild the model exactly as it was when trained. \n",
    "# Information about the model architecture needs to be saved in the checkpoint,\n",
    "# along with the state dict. \n",
    "# To do this, we build a dictionary with all the information \n",
    "# we need to completely rebuild the model. \n",
    "\n",
    "checkpoint = {'input_size': 784,\n",
    "             'output_size': 10,\n",
    "             'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "             'state_dict': model.state_dict()}\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = Network(checkpoint['input_size'],\n",
    "                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
